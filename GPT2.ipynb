{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d517db2a-a020-4a44-b95e-2ceea3101369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6a65fbed144234bf747919ed6245b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jay\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78883b6d22274d8d94199da23591ffb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0223fa66594e9b813b066653712f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e01d7bb93940e1a2dc3085809b3e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136c74bf9dc948fcb703b34f3e14688f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd24471-2c6e-4945-8989-ea935e9376f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  40, 5896, 6918]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"I Love movies\"\n",
    "input1=tokenizer(prompt,return_tensors='pt')\n",
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f2c169-6704-4fdd-b849-bc5963d7385d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ĠLove', 'Ġmovies']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input1['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479ee8d-bf87-41d5-a65b-33c9d1d6cdd4",
   "metadata": {},
   "source": [
    "<b>Here we can note that here there is no CLS and SEP so what I understood was things changes as model changes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427714eb-a41a-44ec-a403-0f3627870518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f51f759cd53493796034dbb9cbae206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f417644e0e3d46458923f6a1e78454cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now lets try GPT2LMHeadModel to predict the next word\n",
    "from transformers import GPT2LMHeadModel\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "output=model(**input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb0d521-3c3f-459c-9be1-e817ef5461c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ -39.3084,  -39.0101,  -41.8375,  ...,  -46.9338,  -44.9074,\n",
       "           -39.5150],\n",
       "         [-102.8635, -102.9315, -108.5859,  ..., -116.8152, -115.4577,\n",
       "          -104.1689],\n",
       "         [-107.1150, -108.4084, -113.4841,  ..., -122.4194, -120.8033,\n",
       "          -109.9530]]], grad_fn=<UnsafeViewBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac42560-5083-448f-bfe7-6fdcfefde9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 50257])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=output.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c0ccb-d670-432d-a9c5-a22acfca1ae5",
   "metadata": {},
   "source": [
    "To get actual next-word probabilities or the predicted token, apply softmax to the last logits (outputs.logits[:, -1, :]), then use argmax for the top prediction or sample from the distribution; this is standard for autoregressive generation in GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caacbd1f-6f4a-4af0-8f66-f37f2bd91866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-107.1150, -108.4084, -113.4841,  ..., -122.4194, -120.8033,\n",
       "         -109.9530]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_logits=logits[:,-1,:]\n",
    "last_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534195cb-c6e0-4141-a8eb-a177f5ddd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_id=torch.argmax(last_token_logits).item()\n",
    "#Here we are saving the next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c627f856-786d-467b-9da3-e90416c8ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now to convert ID to actual word we need to use decode\n",
    "tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4e60bdd-8893-4ea8-806c-6184bc7f6268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "def next_word_predictor(prompt):\n",
    "    input2=tokenizer(prompt,return_tensors='pt')\n",
    "    output=model(**input2)\n",
    "    logits=output.logits\n",
    "    last_token_logits=logits[:,-1,:]\n",
    "    next_token_id=torch.argmax(last_token_logits).item()\n",
    "    return tokenizer.decode(next_token_id)\n",
    "print(next_word_predictor('Apple a day keeps getting better and better for the better.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd3e15-ade1-4507-9a83-e4b52b7ec649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
